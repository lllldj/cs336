{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b234aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "546e51c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toy_softmax(x):\n",
    "    sx = x -  x.max(-1,keepdim=True).values\n",
    "    ex = sx.exp()\n",
    "    return ex/ex.sum(-1,keepdim=True)\n",
    "\n",
    "def toy_product_atte(Q, K, V, mask=None):\n",
    "    d_k = torch.tensor(Q.shape[-1]) \n",
    "    Qk = Q @ K.transpose(-2,-1)/ torch.sqrt(d_k)   \n",
    "    if mask is not None:\n",
    "        Qk = Qk.masked_fill(mask==False,float('-inf'))\n",
    "    sQk = toy_softmax(Qk)\n",
    "    return sQk @ V\n",
    "\n",
    "def toy_multihead_atte(d_model,num_heads,Qp,Kp,Vp,proj,in_features,posistion=None):\n",
    "    Qs = (in_features @ Qp.transpose(-2,-1)).split(d_model//num_heads,-1)\n",
    "    Ks = (in_features @ Kp.transpose(-2,-1)).split(d_model//num_heads,-1)\n",
    "    Vs = (in_features @ Vp.transpose(-2,-1)).split(d_model//num_heads,-1)\n",
    "    \n",
    "    seq_len = Qs[0].size(-2)\n",
    "    mask = torch.tril(torch.ones(seq_len,seq_len))\n",
    "    \n",
    "    atts = [toy_product_atte(Qs[i],Ks[i],Vs[i],mask) for i in range(num_heads)]\n",
    "    atts = torch.cat(atts,-1)\n",
    "    return atts @  proj.transpose(-2,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e493272e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class toy_Liner(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=None, device=None, dtype=torch.float32):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.empty(out_features, in_features,dtype=dtype))\n",
    "        self.bias = nn.Parameter(torch.empty(out_features,dtype=dtype)) if bias else None\n",
    "        self.device = device\n",
    "        \n",
    "        self.set_weights()\n",
    "    \n",
    "    def set_weights(self,w=None):\n",
    "        if w == None:\n",
    "            nn.init.trunc_normal_(self.weight)\n",
    "        else:\n",
    "            self.weight.data = w\n",
    "    \n",
    "    def forward(self,x):\n",
    "        out = x @ self.weight.transpose(-2,-1)\n",
    "        if self.bias != None:\n",
    "            out += self.bias\n",
    "        return out\n",
    "    \n",
    "\n",
    "class toy_Embedding(nn.Module):\n",
    "    def __init__(self, num_embd, embd_dim, device = None,dtype = torch.float32) -> None:\n",
    "        super().__init__()\n",
    "        self.embd = nn.Parameter(torch.empty(num_embd,embd_dim,dtype=dtype))\n",
    "        self.device = device\n",
    "\n",
    "        self.set_para()\n",
    "        \n",
    "    def set_para(self,embd=None):\n",
    "        if embd == None:\n",
    "            nn.init.trunc_normal_(self.embd)\n",
    "        else:\n",
    "            self.embd.data = embd \n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.embd[x]\n",
    "    \n",
    "class toy_RMSnorm(nn.Module):\n",
    "    def __init__(self, d_model, eps: float = 1e-5, device = None, dtype = torch.float32):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.eps = eps\n",
    "        self.gain = nn.Parameter(torch.empty(d_model,dtype=dtype))\n",
    "        self.device = device\n",
    "\n",
    "        self.set_para()\n",
    "        \n",
    "    def set_para(self,g=None):\n",
    "        if g==None:\n",
    "            nn.init.trunc_normal_(self.gain,1,0.02)\n",
    "        else:\n",
    "            self.gain.data = g\n",
    "    \n",
    "    def forward(self,x):\n",
    "        rmsx = x.square().mean(-1,keepdim=True) \n",
    "        out = x*self.gain/torch.sqrt(rmsx+self.eps)\n",
    "        return out \n",
    "        \n",
    "        \n",
    "class toy_SwiGLU(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, device=None, dtype=torch.float32):\n",
    "        super().__init__()\n",
    "        self.W1 = nn.Parameter(torch.empty(d_ff,d_model,dtype=dtype))\n",
    "        self.W2 = nn.Parameter(torch.empty(d_model,d_ff,dtype=dtype))\n",
    "        self.W3 = nn.Parameter(torch.empty(d_ff,d_model,dtype=dtype))\n",
    "        self.device = device\n",
    "    \n",
    "        self.set_para()\n",
    "    def set_para(self,w1=None,w2=None,w3=None):\n",
    "        if w1 == None:\n",
    "            nn.init.trunc_normal_(self.W1)\n",
    "        else:\n",
    "            self.W1.data = w1\n",
    "        if w2 == None:\n",
    "            nn.init.trunc_normal_(self.W2)\n",
    "        else:\n",
    "            self.W2.data = w2\n",
    "        if w3 == None:\n",
    "            nn.init.trunc_normal_(self.W3)\n",
    "        else:\n",
    "            self.W3.data = w3\n",
    "    \n",
    "    def forward(self,x):\n",
    "        W3x = x @ self.W3.transpose(-2,-1)\n",
    "        W1x = x @ self.W1.transpose(-2,-1)\n",
    "        Slu = W1x * torch.sigmoid(W1x)\n",
    "        return (Slu * W3x)@self.W2.transpose(-2,-1)\n",
    "\n",
    "class toy_RoPE(nn.Module):\n",
    "    def __init__(self, d_k, theta, max_len, device = None, dtype = torch.float32):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.rot_d = d_k//2\n",
    "        i = torch.arange(self.rot_d, device=device, dtype=dtype)         \n",
    "        j = torch.arange(max_len, device=device, dtype=dtype)      \n",
    "\n",
    "        inv_freq = torch.exp(-(2*i)/d_k * torch.log(torch.tensor(theta, device=device, dtype=dtype)))                   \n",
    "        thetas = j[:, None] * inv_freq[None, :]  \n",
    "        \n",
    "        cos_table = torch.cos(thetas)  #cos_table [token posistion, feature posistion]\n",
    "        sin_table = torch.sin(thetas)\n",
    "        \n",
    "        self.register_buffer(\"cos_table\",cos_table,persistent=False)\n",
    "        self.register_buffer(\"sin_table\",sin_table,persistent=False)\n",
    "    \n",
    "    def forward(self,x,tk_posistions):\n",
    "        cos = self.cos_table[tk_posistions] #(T,d/2)\n",
    "        sin = self.sin_table[tk_posistions] #(T,d/2)\n",
    "        \n",
    "        \n",
    "        x_rot = x[..., :2*self.rot_d]\n",
    "        x_pass = x[..., 2*self.rot_d:]\n",
    "        x1 = x_rot[...,0::2] #(T,d/2 + 1) ?\n",
    "        x2 = x_rot[...,1::2]\n",
    "        y1 = x1 * cos - x2 * sin\n",
    "        y2 = x1 * sin + x2 * cos\n",
    "        y_rot = torch.stack([y1, y2], dim=-1).flatten(-2)\n",
    "        return torch.cat([y_rot, x_pass], dim=-1)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3eac269",
   "metadata": {},
   "outputs": [],
   "source": [
    "class multi_attention(nn.Module):\n",
    "    def __init__(self, d_in, num_heads, max_seq_len, theta, device = None) -> None:\n",
    "        super().__init__()\n",
    "        self.c_attention = toy_Liner(d_in, 3* d_in)\n",
    "        self.proj = toy_Liner(d_in,d_in)\n",
    "        self.num_head = num_heads\n",
    "        self.d_head = d_in//self.num_head       \n",
    "        self.ropez = toy_RoPE(self.d_head, theta ,max_seq_len)\n",
    "        self.device = device\n",
    "        \n",
    "        trill_mask = torch.tril(torch.ones(max_seq_len,max_seq_len,dtype=torch.bool))\n",
    "        self.register_buffer(\"trill\",trill_mask,persistent=False)\n",
    "    \n",
    "    \n",
    "    def forward(self,x):\n",
    "        B, T, C = x.shape\n",
    "        qkv = self.c_attention(x) #B,T,C @ C 3C -> B,T,3C\n",
    "        Q,K,V = qkv.split(C,-1) # B,T,C\n",
    "        \n",
    "        qs = Q.view(B,T,self.num_head,self.d_head).transpose(1,2)\n",
    "        ks = K.view(B,T,self.num_head,self.d_head).transpose(1,2) #B,h,T,d_h\n",
    "        vs = V.view(B,T,self.num_head,self.d_head).transpose(1,2)\n",
    "        \n",
    "        tk_ps = torch.arange(T)\n",
    "        qs = self.ropez.forward(qs,tk_ps)  \n",
    "        ks = self.ropez.forward(ks,tk_ps)\n",
    "        \n",
    "        atts = toy_product_atte(qs,ks,vs,self.trill[:T,:T]).transpose(1, 2).contiguous().view(B,T,C) # B, T ,C\n",
    "        return self.proj(atts)\n",
    "    \n",
    "class transformer_block(nn.Module):\n",
    "    def __init__(self, d_in, num_heads, d_ff, max_seq_len, theta, device=None) -> None:\n",
    "        super().__init__()   \n",
    "        self.norm1 = toy_RMSnorm(d_in)\n",
    "        self.atte = multi_attention(d_in,num_heads,max_seq_len,theta,device)\n",
    "        self.norm2 = toy_RMSnorm(d_in)\n",
    "        self.ff = toy_SwiGLU(d_in,d_ff)\n",
    "        self.device = device\n",
    "    \n",
    "    def set_para(self,para_dict):\n",
    "        q_proj_weight = para_dict[\"attn.q_proj.weight\"]\n",
    "        k_proj_weight = para_dict[\"attn.k_proj.weight\"]\n",
    "        v_proj_weight = para_dict[\"attn.v_proj.weight\"]\n",
    "        o_proj_weight = para_dict[\"attn.output_proj.weight\"]\n",
    "        ln1_weight = para_dict[\"ln1.weight\"]\n",
    "        ln2_weight = para_dict[\"ln2.weight\"]\n",
    "        ff_w1 = para_dict[\"ffn.w1.weight\"]\n",
    "        ff_w2 = para_dict[\"ffn.w2.weight\"]\n",
    "        ff_w3 = para_dict[\"ffn.w3.weight\"]\n",
    "        c_atte_weight = torch.cat([q_proj_weight,k_proj_weight,v_proj_weight],0)\n",
    "        self.atte.c_attention.data = c_atte_weight\n",
    "        self.atte.proj.data = o_proj_weight\n",
    "        self.norm1.set_para(ln1_weight)\n",
    "        self.norm2.set_para(ln2_weight)\n",
    "        self.ff.set_para(ff_w1,ff_w2,ff_w3)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = x + self.atte(self.norm1(x))\n",
    "        x = x + self.ff(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa038c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "class toy_Transformer_lm(nn.Module):\n",
    "    def __init__(self, vocab_size, context_length, d_model, num_layers, num_heads, d_ff, rope_theta, device = None) -> None:\n",
    "        super().__init__()\n",
    "        self.tk_embd = toy_Embedding(vocab_size,d_model)\n",
    "        self.blocks =nn.ModuleList([transformer_block(d_model,num_heads,d_ff,context_length,rope_theta) for _ in range(num_layers)])\n",
    "        self.norm = toy_RMSnorm(d_model)\n",
    "        self.out_embd = toy_Liner(d_model,vocab_size)\n",
    "    \n",
    "    def set_para(self,para_dict):\n",
    "        self.tk_embd.set_para(para_dict[\"token_embeddings.weight\"])\n",
    "        self.out_embd.set_weights(para_dict[\"lm_head.weight\"])\n",
    "        self.norm.set_para(para_dict[\"ln_final.weight\"])\n",
    "        grouped = defaultdict(dict)\n",
    "        pat = re.compile(r\"^layers\\.(\\d+)\\.(.+)$\")  # layers.{i}.rest\n",
    "\n",
    "        for k, v in para_dict.items():\n",
    "            m = pat.match(k)\n",
    "            if m:\n",
    "                i = int(m.group(1))\n",
    "                rest = m.group(2)  # e.g. \"ffn.w3.weight\"\n",
    "                grouped[i][rest] = v\n",
    "        layer_dict = dict(grouped)\n",
    "        for _ ,blk in enumerate(self.blocks):\n",
    "            blk.set_para(layer_dict[_])\n",
    "    def forward(self,x):\n",
    "        #x : ids\n",
    "        x = self.tk_embd(x)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "        return self.out_embd(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eba73c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  5.0490,   9.1225,   4.8232,   3.0160, -12.8134,  -0.6967,  -6.0283,\n",
       "          -10.6801,   0.1312,   7.3410],\n",
       "         [  6.1181,   7.3358,  -4.6733,   5.7041,  -6.1997,  -6.0524,  -5.4670,\n",
       "            0.2124,   1.9641,   1.8383],\n",
       "         [ 10.8416,   9.0004,   8.1528,   1.5137, -13.1621,   6.4140,  -8.8128,\n",
       "           -4.4009,   5.0310,  -1.4037],\n",
       "         [  0.6109,  -1.7073, -12.9099,   4.9808,  13.2151, -10.3835,   1.6771,\n",
       "           10.4893,  -8.0457, -10.0227],\n",
       "         [-11.4117,  -8.4786, -12.5582,   7.1547,   8.3548,  -8.7325,   2.2082,\n",
       "            2.9028,  18.2975,  -7.4372]],\n",
       "\n",
       "        [[ -2.0266,  12.7991,   3.1344,   1.3523, -19.5812,  -9.1501,  -3.1588,\n",
       "          -19.6919,   5.2371,   8.3103],\n",
       "         [  2.2641,   5.2216,   1.3730,   6.4510, -11.3216,  -2.9140,  -1.9544,\n",
       "           -6.0966,   8.3469,  -3.0424],\n",
       "         [  7.8703,  15.6284,  24.5607,  -9.3389,  -2.4780,  11.6780,  -3.2490,\n",
       "           -7.7913,  -3.9369,  14.9561],\n",
       "         [  3.2591,   2.8341,  -8.9729,  10.3043, -16.4635,  -9.4049,  -2.6320,\n",
       "           -2.7330,   2.5394,  -2.0230],\n",
       "         [  5.2273,  15.4161,  12.6446, -11.3183,   9.9811,   2.5890,  -2.0800,\n",
       "            1.7915,  -3.7258,  10.3019]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_h = multi_attention(10,2,10,0.0001)\n",
    "a = torch.randn(2,5,10)\n",
    "test_h(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f1df64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def toy_multihead_atte_rope(d_model: int,\n",
    "    num_heads: int,\n",
    "    max_seq_len: int,\n",
    "    theta: float,\n",
    "    Qp, #Float[Tensor, \" d_k d_in\"],\n",
    "    Kp, #Float[Tensor, \" d_k d_in\"],\n",
    "    Vp, #Float[Tensor, \" d_v d_in\"],\n",
    "    proj, #Float[Tensor, \" d_model d_v\"],\n",
    "    in_features, #Float[Tensor, \" ... sequence_length d_in\"],\n",
    "    token_positions, #Int[Tensor, \" ... sequence_length\"] | None = None,\n",
    "):# -> Float[Tensor, \" ... sequence_length d_out\"]\n",
    "    Ro = toy_RoPE(d_model//num_heads,theta,max_seq_len) #\n",
    "    Qs = (in_features @ Qp.transpose(-2,-1)).split(d_model//num_heads,-1)\n",
    "    Ks = (in_features @ Kp.transpose(-2,-1)).split(d_model//num_heads,-1)\n",
    "    Vs = (in_features @ Vp.transpose(-2,-1)).split(d_model//num_heads,-1)\n",
    "    \n",
    "    Qs = [Ro.forward(Qs[i],token_positions) for i in range(num_heads)] #\n",
    "    Ks = [Ro.forward(Ks[i],token_positions) for i in range(num_heads)]\n",
    "    \n",
    "    seq_len = Qs[0].size(-2)\n",
    "    mask = torch.tril(torch.ones(seq_len,seq_len))\n",
    "    atts = [toy_product_atte(Qs[i],Ks[i],Vs[i],mask) for i in range(num_heads)]\n",
    "    atts = torch.cat(atts,-1)\n",
    "    return atts @  proj.transpose(-2,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6441bba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336-basics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
