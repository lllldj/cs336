{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbea52ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "import os\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from typing import BinaryIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5d7724f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_path = \"/root/workspace/cs336/assignment1/tests/fixtures/corpus.en\"\n",
    "merge_file_path = \"/root/workspace/cs336/assignment1/my_mergeslist.txt\"\n",
    "vocab_file_path = \"/root/workspace/cs336/assignment1/my_vocab.txt\"\n",
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "vocab_size = 500\n",
    "special_tokens = [\"<|endoftext|>\"]\n",
    "num_processes = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5d26335",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def find_chunk_boundaries(\n",
    "    file: BinaryIO,\n",
    "    desired_num_chunks: int,\n",
    "    split_special_token: bytes,\n",
    ") -> list[int]:\n",
    "    \"\"\"\n",
    "    Chunk the file into parts that can be counted independently.\n",
    "    May return fewer chunks if the boundaries end up overlapping.\n",
    "    \"\"\"\n",
    "    assert isinstance(split_special_token, bytes), \"Must represent special token as a bytestring\"\n",
    "\n",
    "    # Get total file size in bytes\n",
    "    file.seek(0, os.SEEK_END)\n",
    "    file_size = file.tell()\n",
    "    file.seek(0)\n",
    "\n",
    "    chunk_size = file_size // desired_num_chunks\n",
    "\n",
    "    # Initial guesses for chunk boundary locations, uniformly spaced\n",
    "    # Chunks start on previous index, don't include last index\n",
    "    chunk_boundaries = [i * chunk_size for i in range(desired_num_chunks + 1)]\n",
    "    chunk_boundaries[-1] = file_size\n",
    "\n",
    "    mini_chunk_size = 4096  # Read ahead by 4k bytes at a time\n",
    "\n",
    "    for bi in range(1, len(chunk_boundaries) - 1):\n",
    "        initial_position = chunk_boundaries[bi]\n",
    "        file.seek(initial_position)  # Start at boundary guess\n",
    "        while True:\n",
    "            mini_chunk = file.read(mini_chunk_size)  # Read a mini chunk\n",
    "\n",
    "            # If EOF, this boundary should be at the end of the file\n",
    "            if mini_chunk == b\"\":\n",
    "                chunk_boundaries[bi] = file_size\n",
    "                break\n",
    "\n",
    "            # Find the special token in the mini chunk\n",
    "            found_at = mini_chunk.find(split_special_token)\n",
    "            if found_at != -1:\n",
    "                chunk_boundaries[bi] = initial_position + found_at\n",
    "                break\n",
    "            initial_position += mini_chunk_size\n",
    "\n",
    "    # Make sure all boundaries are unique, but might be fewer than desired_num_chunks\n",
    "    return sorted(set(chunk_boundaries))\n",
    "\n",
    "def pre_tokenize_split(filepath, bound_st, bound_ed, pattern, special_tokens):\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        f.seek(bound_st)\n",
    "        chunk = f.read(bound_ed - bound_st).decode(\"utf-8\", errors=\"ignore\")\n",
    "        special_pat = \"|\".join(map(re.escape, special_tokens))\n",
    "        chunk_set = [s for s in re.split(special_pat, chunk) if s]\n",
    "        corpus_weights = {}\n",
    "        for small_chunk in chunk_set:\n",
    "            splited_text = re.findall(pattern, small_chunk)\n",
    "            for words in splited_text:\n",
    "                data_u8 = words.encode(\"utf-8\")\n",
    "                corpus_weights[data_u8] = corpus_weights.get(data_u8, 0) + 1\n",
    "    return corpus_weights\n",
    "\n",
    "def build_seq_weights(filepath, num_process, special_tokens, PAT):\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        boundaries = find_chunk_boundaries(f, num_processes, b\"<|endoftext|>\")\n",
    "        \n",
    "    parellel_params = [(filepath, start, end, PAT, special_tokens) for start, end in zip(boundaries[:-1], boundaries[1:])]\n",
    "    \n",
    "    with ProcessPoolExecutor(max_workers=num_process) as ex:\n",
    "        results = list(ex.map(pre_tokenize_split, *zip(*parellel_params)))\n",
    "        \n",
    "    seq_weights = {} #{tuple: int(freq)}\n",
    "\n",
    "    for dic in results:\n",
    "        for k,v in dic.items():\n",
    "            tuple_k = tuple(k)\n",
    "            seq_weights[tuple_k] = seq_weights.get(tuple_k,0) + v\n",
    "    \n",
    "    return seq_weights\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4054022a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_pair_cnt(_seq_weights):\n",
    "    _pair_cnt = {}\n",
    "    for k,v in _seq_weights.items():\n",
    "        for ch1,ch2 in zip(k[:-1],k[1:]):\n",
    "            pair = (ch1,ch2)\n",
    "            _pair_cnt[pair] = _pair_cnt.get(pair,0) + v\n",
    "    return _pair_cnt\n",
    "\n",
    "def find_max(_pair_cnt,_token_dict):\n",
    "    maxcnt = -1\n",
    "    maxpair = None\n",
    "    bytes_pair = None\n",
    "    for p,v in _pair_cnt.items():\n",
    "        if v == maxcnt:\n",
    "            bytes_pair_new = _token_dict[p[0]],_token_dict[p[1]]\n",
    "            if bytes_pair_new > bytes_pair:\n",
    "                bytes_pair = bytes_pair_new\n",
    "                maxpair = p\n",
    "        if v > maxcnt:\n",
    "            maxcnt = v\n",
    "            maxpair = p\n",
    "            bytes_pair = _token_dict[p[0]],_token_dict[p[1]]\n",
    "    return maxpair,maxcnt\n",
    "\n",
    "def merge_operation(_seq_weights, merge_pair, merge_id):\n",
    "    _seq_weights_copy = {}\n",
    "    for k, v in _seq_weights.items():\n",
    "        _ = 0\n",
    "        new_k = []\n",
    "        while _ < len(k):\n",
    "            if _ + 1 < len(k) and (k[_],k[_+1])==merge_pair:\n",
    "                new_k.append(merge_id)\n",
    "                _ += 2\n",
    "            else:\n",
    "                new_k.append(k[_])\n",
    "                _ += 1\n",
    "        new_k = tuple(new_k)\n",
    "        _seq_weights_copy[new_k] = _seq_weights_copy.get(new_k,0)+v\n",
    "    return _seq_weights_copy\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "43746927",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_train_bpe(filepath,vocab_size,special_tokens,PAT,num_processes=4):\n",
    "    token_dict = {i:bytes([i]) for i in range(256)}\n",
    "    merge_list = []\n",
    "    seq_now = build_seq_weights(filepath,num_processes,special_tokens,PAT)\n",
    "    merge_num = vocab_size - len(special_tokens) - 256\n",
    "    token_id = 256\n",
    "    for i in range(merge_num):\n",
    "        pairnow,freq = find_max(build_pair_cnt(seq_now),token_dict)\n",
    "        seq_now = merge_operation(seq_now,pairnow,token_id)\n",
    "        token_dict[token_id] = token_dict[pairnow[0]]+token_dict[pairnow[1]] \n",
    "        merge_list.append((token_dict[pairnow[0]],token_dict[pairnow[1]]))\n",
    "        token_id += 1\n",
    "    for s in special_tokens:\n",
    "        token_dict[token_id] = bytes(s.encode(\"utf-8\"))\n",
    "        token_id += 1\n",
    "    return token_dict, merge_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bc33b121",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_dict,merge_list = my_train_bpe(train_text_path,vocab_size,special_tokens,PAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4daf0cea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: b'\\x00',\n",
       " 1: b'\\x01',\n",
       " 2: b'\\x02',\n",
       " 3: b'\\x03',\n",
       " 4: b'\\x04',\n",
       " 5: b'\\x05',\n",
       " 6: b'\\x06',\n",
       " 7: b'\\x07',\n",
       " 8: b'\\x08',\n",
       " 9: b'\\t',\n",
       " 10: b'\\n',\n",
       " 11: b'\\x0b',\n",
       " 12: b'\\x0c',\n",
       " 13: b'\\r',\n",
       " 14: b'\\x0e',\n",
       " 15: b'\\x0f',\n",
       " 16: b'\\x10',\n",
       " 17: b'\\x11',\n",
       " 18: b'\\x12',\n",
       " 19: b'\\x13',\n",
       " 20: b'\\x14',\n",
       " 21: b'\\x15',\n",
       " 22: b'\\x16',\n",
       " 23: b'\\x17',\n",
       " 24: b'\\x18',\n",
       " 25: b'\\x19',\n",
       " 26: b'\\x1a',\n",
       " 27: b'\\x1b',\n",
       " 28: b'\\x1c',\n",
       " 29: b'\\x1d',\n",
       " 30: b'\\x1e',\n",
       " 31: b'\\x1f',\n",
       " 32: b' ',\n",
       " 33: b'!',\n",
       " 34: b'\"',\n",
       " 35: b'#',\n",
       " 36: b'$',\n",
       " 37: b'%',\n",
       " 38: b'&',\n",
       " 39: b\"'\",\n",
       " 40: b'(',\n",
       " 41: b')',\n",
       " 42: b'*',\n",
       " 43: b'+',\n",
       " 44: b',',\n",
       " 45: b'-',\n",
       " 46: b'.',\n",
       " 47: b'/',\n",
       " 48: b'0',\n",
       " 49: b'1',\n",
       " 50: b'2',\n",
       " 51: b'3',\n",
       " 52: b'4',\n",
       " 53: b'5',\n",
       " 54: b'6',\n",
       " 55: b'7',\n",
       " 56: b'8',\n",
       " 57: b'9',\n",
       " 58: b':',\n",
       " 59: b';',\n",
       " 60: b'<',\n",
       " 61: b'=',\n",
       " 62: b'>',\n",
       " 63: b'?',\n",
       " 64: b'@',\n",
       " 65: b'A',\n",
       " 66: b'B',\n",
       " 67: b'C',\n",
       " 68: b'D',\n",
       " 69: b'E',\n",
       " 70: b'F',\n",
       " 71: b'G',\n",
       " 72: b'H',\n",
       " 73: b'I',\n",
       " 74: b'J',\n",
       " 75: b'K',\n",
       " 76: b'L',\n",
       " 77: b'M',\n",
       " 78: b'N',\n",
       " 79: b'O',\n",
       " 80: b'P',\n",
       " 81: b'Q',\n",
       " 82: b'R',\n",
       " 83: b'S',\n",
       " 84: b'T',\n",
       " 85: b'U',\n",
       " 86: b'V',\n",
       " 87: b'W',\n",
       " 88: b'X',\n",
       " 89: b'Y',\n",
       " 90: b'Z',\n",
       " 91: b'[',\n",
       " 92: b'\\\\',\n",
       " 93: b']',\n",
       " 94: b'^',\n",
       " 95: b'_',\n",
       " 96: b'`',\n",
       " 97: b'a',\n",
       " 98: b'b',\n",
       " 99: b'c',\n",
       " 100: b'd',\n",
       " 101: b'e',\n",
       " 102: b'f',\n",
       " 103: b'g',\n",
       " 104: b'h',\n",
       " 105: b'i',\n",
       " 106: b'j',\n",
       " 107: b'k',\n",
       " 108: b'l',\n",
       " 109: b'm',\n",
       " 110: b'n',\n",
       " 111: b'o',\n",
       " 112: b'p',\n",
       " 113: b'q',\n",
       " 114: b'r',\n",
       " 115: b's',\n",
       " 116: b't',\n",
       " 117: b'u',\n",
       " 118: b'v',\n",
       " 119: b'w',\n",
       " 120: b'x',\n",
       " 121: b'y',\n",
       " 122: b'z',\n",
       " 123: b'{',\n",
       " 124: b'|',\n",
       " 125: b'}',\n",
       " 126: b'~',\n",
       " 127: b'\\x7f',\n",
       " 128: b'\\x80',\n",
       " 129: b'\\x81',\n",
       " 130: b'\\x82',\n",
       " 131: b'\\x83',\n",
       " 132: b'\\x84',\n",
       " 133: b'\\x85',\n",
       " 134: b'\\x86',\n",
       " 135: b'\\x87',\n",
       " 136: b'\\x88',\n",
       " 137: b'\\x89',\n",
       " 138: b'\\x8a',\n",
       " 139: b'\\x8b',\n",
       " 140: b'\\x8c',\n",
       " 141: b'\\x8d',\n",
       " 142: b'\\x8e',\n",
       " 143: b'\\x8f',\n",
       " 144: b'\\x90',\n",
       " 145: b'\\x91',\n",
       " 146: b'\\x92',\n",
       " 147: b'\\x93',\n",
       " 148: b'\\x94',\n",
       " 149: b'\\x95',\n",
       " 150: b'\\x96',\n",
       " 151: b'\\x97',\n",
       " 152: b'\\x98',\n",
       " 153: b'\\x99',\n",
       " 154: b'\\x9a',\n",
       " 155: b'\\x9b',\n",
       " 156: b'\\x9c',\n",
       " 157: b'\\x9d',\n",
       " 158: b'\\x9e',\n",
       " 159: b'\\x9f',\n",
       " 160: b'\\xa0',\n",
       " 161: b'\\xa1',\n",
       " 162: b'\\xa2',\n",
       " 163: b'\\xa3',\n",
       " 164: b'\\xa4',\n",
       " 165: b'\\xa5',\n",
       " 166: b'\\xa6',\n",
       " 167: b'\\xa7',\n",
       " 168: b'\\xa8',\n",
       " 169: b'\\xa9',\n",
       " 170: b'\\xaa',\n",
       " 171: b'\\xab',\n",
       " 172: b'\\xac',\n",
       " 173: b'\\xad',\n",
       " 174: b'\\xae',\n",
       " 175: b'\\xaf',\n",
       " 176: b'\\xb0',\n",
       " 177: b'\\xb1',\n",
       " 178: b'\\xb2',\n",
       " 179: b'\\xb3',\n",
       " 180: b'\\xb4',\n",
       " 181: b'\\xb5',\n",
       " 182: b'\\xb6',\n",
       " 183: b'\\xb7',\n",
       " 184: b'\\xb8',\n",
       " 185: b'\\xb9',\n",
       " 186: b'\\xba',\n",
       " 187: b'\\xbb',\n",
       " 188: b'\\xbc',\n",
       " 189: b'\\xbd',\n",
       " 190: b'\\xbe',\n",
       " 191: b'\\xbf',\n",
       " 192: b'\\xc0',\n",
       " 193: b'\\xc1',\n",
       " 194: b'\\xc2',\n",
       " 195: b'\\xc3',\n",
       " 196: b'\\xc4',\n",
       " 197: b'\\xc5',\n",
       " 198: b'\\xc6',\n",
       " 199: b'\\xc7',\n",
       " 200: b'\\xc8',\n",
       " 201: b'\\xc9',\n",
       " 202: b'\\xca',\n",
       " 203: b'\\xcb',\n",
       " 204: b'\\xcc',\n",
       " 205: b'\\xcd',\n",
       " 206: b'\\xce',\n",
       " 207: b'\\xcf',\n",
       " 208: b'\\xd0',\n",
       " 209: b'\\xd1',\n",
       " 210: b'\\xd2',\n",
       " 211: b'\\xd3',\n",
       " 212: b'\\xd4',\n",
       " 213: b'\\xd5',\n",
       " 214: b'\\xd6',\n",
       " 215: b'\\xd7',\n",
       " 216: b'\\xd8',\n",
       " 217: b'\\xd9',\n",
       " 218: b'\\xda',\n",
       " 219: b'\\xdb',\n",
       " 220: b'\\xdc',\n",
       " 221: b'\\xdd',\n",
       " 222: b'\\xde',\n",
       " 223: b'\\xdf',\n",
       " 224: b'\\xe0',\n",
       " 225: b'\\xe1',\n",
       " 226: b'\\xe2',\n",
       " 227: b'\\xe3',\n",
       " 228: b'\\xe4',\n",
       " 229: b'\\xe5',\n",
       " 230: b'\\xe6',\n",
       " 231: b'\\xe7',\n",
       " 232: b'\\xe8',\n",
       " 233: b'\\xe9',\n",
       " 234: b'\\xea',\n",
       " 235: b'\\xeb',\n",
       " 236: b'\\xec',\n",
       " 237: b'\\xed',\n",
       " 238: b'\\xee',\n",
       " 239: b'\\xef',\n",
       " 240: b'\\xf0',\n",
       " 241: b'\\xf1',\n",
       " 242: b'\\xf2',\n",
       " 243: b'\\xf3',\n",
       " 244: b'\\xf4',\n",
       " 245: b'\\xf5',\n",
       " 246: b'\\xf6',\n",
       " 247: b'\\xf7',\n",
       " 248: b'\\xf8',\n",
       " 249: b'\\xf9',\n",
       " 250: b'\\xfa',\n",
       " 251: b'\\xfb',\n",
       " 252: b'\\xfc',\n",
       " 253: b'\\xfd',\n",
       " 254: b'\\xfe',\n",
       " 255: b'\\xff',\n",
       " 256: b' t',\n",
       " 257: b' a',\n",
       " 258: b'he',\n",
       " 259: b'in',\n",
       " 260: b' the',\n",
       " 261: b're',\n",
       " 262: b' o',\n",
       " 263: b' ,',\n",
       " 264: b'er',\n",
       " 265: b' s',\n",
       " 266: b'at',\n",
       " 267: b' .',\n",
       " 268: b'nd',\n",
       " 269: b'is',\n",
       " 270: b'or',\n",
       " 271: b' w',\n",
       " 272: b' c',\n",
       " 273: b'on',\n",
       " 274: b' b',\n",
       " 275: b' f',\n",
       " 276: b'ou',\n",
       " 277: b'it',\n",
       " 278: b'en',\n",
       " 279: b'es',\n",
       " 280: b' of',\n",
       " 281: b' p',\n",
       " 282: b'ing',\n",
       " 283: b' in',\n",
       " 284: b'ed',\n",
       " 285: b'al',\n",
       " 286: b' m',\n",
       " 287: b' and',\n",
       " 288: b' d',\n",
       " 289: b'an',\n",
       " 290: b'ar',\n",
       " 291: b' to',\n",
       " 292: b'om',\n",
       " 293: b' th',\n",
       " 294: b'ic',\n",
       " 295: b'ion',\n",
       " 296: b' h',\n",
       " 297: b' l',\n",
       " 298: b' y',\n",
       " 299: b' e',\n",
       " 300: b'as',\n",
       " 301: b'ot',\n",
       " 302: b'il',\n",
       " 303: b' n',\n",
       " 304: b' u',\n",
       " 305: b'ent',\n",
       " 306: b' be',\n",
       " 307: b' &',\n",
       " 308: b' is',\n",
       " 309: b' you',\n",
       " 310: b'os',\n",
       " 311: b' re',\n",
       " 312: b'et',\n",
       " 313: b' for',\n",
       " 314: b'ut',\n",
       " 315: b'el',\n",
       " 316: b' g',\n",
       " 317: b'ay',\n",
       " 318: b'st',\n",
       " 319: b'ow',\n",
       " 320: b'le',\n",
       " 321: b'ce',\n",
       " 322: b'ad',\n",
       " 323: b' on',\n",
       " 324: b' I',\n",
       " 325: b'ver',\n",
       " 326: b've',\n",
       " 327: b' A',\n",
       " 328: b'ur',\n",
       " 329: b'ol',\n",
       " 330: b'ct',\n",
       " 331: b'qu',\n",
       " 332: b' that',\n",
       " 333: b'im',\n",
       " 334: b'all',\n",
       " 335: b'am',\n",
       " 336: b'ig',\n",
       " 337: b'ch',\n",
       " 338: b'ation',\n",
       " 339: b' P',\n",
       " 340: b'ith',\n",
       " 341: b'ir',\n",
       " 342: b' S',\n",
       " 343: b' it',\n",
       " 344: b' pr',\n",
       " 345: b'ap',\n",
       " 346: b' sh',\n",
       " 347: b' C',\n",
       " 348: b'th',\n",
       " 349: b' com',\n",
       " 350: b' @',\n",
       " 351: b' wh',\n",
       " 352: b'-@',\n",
       " 353: b' are',\n",
       " 354: b' @-@',\n",
       " 355: b'nt',\n",
       " 356: b'id',\n",
       " 357: b' with',\n",
       " 358: b' al',\n",
       " 359: b'op',\n",
       " 360: b' us',\n",
       " 361: b'ers',\n",
       " 362: b' as',\n",
       " 363: b'the',\n",
       " 364: b'and',\n",
       " 365: b'if',\n",
       " 366: b'ord',\n",
       " 367: b'od',\n",
       " 368: b' he',\n",
       " 369: b'ist',\n",
       " 370: b'quot',\n",
       " 371: b'ment',\n",
       " 372: b' M',\n",
       " 373: b' or',\n",
       " 374: b'ore',\n",
       " 375: b' G',\n",
       " 376: b' fr',\n",
       " 377: b'ill',\n",
       " 378: b'res',\n",
       " 379: b' st',\n",
       " 380: b'ess',\n",
       " 381: b'ld',\n",
       " 382: b' this',\n",
       " 383: b' 2',\n",
       " 384: b'art',\n",
       " 385: b' ;',\n",
       " 386: b' L',\n",
       " 387: b'ly',\n",
       " 388: b'ain',\n",
       " 389: b'ul',\n",
       " 390: b' de',\n",
       " 391: b' con',\n",
       " 392: b'est',\n",
       " 393: b'se',\n",
       " 394: b'apos',\n",
       " 395: b'ag',\n",
       " 396: b' from',\n",
       " 397: b' an',\n",
       " 398: b' we',\n",
       " 399: b' (',\n",
       " 400: b'00',\n",
       " 401: b'ter',\n",
       " 402: b' E',\n",
       " 403: b'em',\n",
       " 404: b'ave',\n",
       " 405: b' not',\n",
       " 406: b' )',\n",
       " 407: b' 1',\n",
       " 408: b' your',\n",
       " 409: b' can',\n",
       " 410: b'oc',\n",
       " 411: b' by',\n",
       " 412: b' D',\n",
       " 413: b' ne',\n",
       " 414: b' v',\n",
       " 415: b'igh',\n",
       " 416: b'ich',\n",
       " 417: b' all',\n",
       " 418: b'ri',\n",
       " 419: b' up',\n",
       " 420: b' r',\n",
       " 421: b' W',\n",
       " 422: b'ble',\n",
       " 423: b' they',\n",
       " 424: b' B',\n",
       " 425: b' ye',\n",
       " 426: b'un',\n",
       " 427: b' which',\n",
       " 428: b' O',\n",
       " 429: b'ke',\n",
       " 430: b' wor',\n",
       " 431: b' su',\n",
       " 432: b' F',\n",
       " 433: b' H',\n",
       " 434: b' have',\n",
       " 435: b' shall',\n",
       " 436: b'ate',\n",
       " 437: b' ch',\n",
       " 438: b'ect',\n",
       " 439: b'ity',\n",
       " 440: b' sp',\n",
       " 441: b'ight',\n",
       " 442: b'ress',\n",
       " 443: b' will',\n",
       " 444: b' comp',\n",
       " 445: b'ort',\n",
       " 446: b'ant',\n",
       " 447: b' &#',\n",
       " 448: b'ive',\n",
       " 449: b'are',\n",
       " 450: b'..',\n",
       " 451: b' And',\n",
       " 452: b' ex',\n",
       " 453: b' ...',\n",
       " 454: b'ast',\n",
       " 455: b'12',\n",
       " 456: b' T',\n",
       " 457: b'ould',\n",
       " 458: b'ven',\n",
       " 459: b' tr',\n",
       " 460: b'ust',\n",
       " 461: b'um',\n",
       " 462: b'out',\n",
       " 463: b' unt',\n",
       " 464: b'com',\n",
       " 465: b' se',\n",
       " 466: b'ft',\n",
       " 467: b'lo',\n",
       " 468: b'ree',\n",
       " 469: b'ost',\n",
       " 470: b'ish',\n",
       " 471: b'ions',\n",
       " 472: b' unto',\n",
       " 473: b'124',\n",
       " 474: b'iz',\n",
       " 475: b' pro',\n",
       " 476: b'mer',\n",
       " 477: b'ings',\n",
       " 478: b' ac',\n",
       " 479: b'ated',\n",
       " 480: b'this',\n",
       " 481: b'ac',\n",
       " 482: b'lu',\n",
       " 483: b'ere',\n",
       " 484: b' man',\n",
       " 485: b'for',\n",
       " 486: b' my',\n",
       " 487: b' at',\n",
       " 488: b'ies',\n",
       " 489: b'age',\n",
       " 490: b'rou',\n",
       " 491: b'ans',\n",
       " 492: b'ind',\n",
       " 493: b'pp',\n",
       " 494: b' work',\n",
       " 495: b'here',\n",
       " 496: b'fore',\n",
       " 497: b' sit',\n",
       " 498: b' ver',\n",
       " 499: b'<|endoftext|>'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c58b79bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests.common import gpt2_bytes_to_unicode\n",
    "reference_merges_path = \"/root/workspace/cs336/assignment1/tests/fixtures/train-bpe-reference-merges.txt\"\n",
    "gpt2_byte_decoder = {v: k for k, v in gpt2_bytes_to_unicode().items()}\n",
    "with open(reference_merges_path, encoding=\"utf-8\") as f:\n",
    "    gpt2_reference_merges = [tuple(line.rstrip().split(\" \")) for line in f]\n",
    "    reference_merges = [\n",
    "        (\n",
    "            bytes([gpt2_byte_decoder[token] for token in merge_token_1]),\n",
    "            bytes([gpt2_byte_decoder[token] for token in merge_token_2]),\n",
    "        )\n",
    "        for merge_token_1, merge_token_2 in gpt2_reference_merges\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5c84f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mymerge,merge in zip(merge_list,reference_merges):\n",
    "    if mymerge!=merge:\n",
    "        print(f\"mymerge:{mymerge},true merge:{merge}\")\n",
    "        print(mymerge>merge)\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336-basics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
