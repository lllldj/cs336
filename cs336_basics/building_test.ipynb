{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbea52ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "import os\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from typing import BinaryIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5d7724f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_path = \"/root/workspace/cs336/assignment1/tests/fixtures/corpus.en\"\n",
    "merge_file_path = \"/root/workspace/cs336/assignment1/my_mergeslist.txt\"\n",
    "vocab_file_path = \"/root/workspace/cs336/assignment1/my_vocab.txt\"\n",
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "vocab_size = 500\n",
    "special_tokens = [\"<|endoftext|>\"]\n",
    "num_processes = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5d26335",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def find_chunk_boundaries(\n",
    "    file: BinaryIO,\n",
    "    desired_num_chunks: int,\n",
    "    split_special_token: bytes,\n",
    ") -> list[int]:\n",
    "    \"\"\"\n",
    "    Chunk the file into parts that can be counted independently.\n",
    "    May return fewer chunks if the boundaries end up overlapping.\n",
    "    \"\"\"\n",
    "    assert isinstance(split_special_token, bytes), \"Must represent special token as a bytestring\"\n",
    "\n",
    "    # Get total file size in bytes\n",
    "    file.seek(0, os.SEEK_END)\n",
    "    file_size = file.tell()\n",
    "    file.seek(0)\n",
    "\n",
    "    chunk_size = file_size // desired_num_chunks\n",
    "\n",
    "    # Initial guesses for chunk boundary locations, uniformly spaced\n",
    "    # Chunks start on previous index, don't include last index\n",
    "    chunk_boundaries = [i * chunk_size for i in range(desired_num_chunks + 1)]\n",
    "    chunk_boundaries[-1] = file_size\n",
    "\n",
    "    mini_chunk_size = 4096  # Read ahead by 4k bytes at a time\n",
    "\n",
    "    for bi in range(1, len(chunk_boundaries) - 1):\n",
    "        initial_position = chunk_boundaries[bi]\n",
    "        file.seek(initial_position)  # Start at boundary guess\n",
    "        while True:\n",
    "            mini_chunk = file.read(mini_chunk_size)  # Read a mini chunk\n",
    "\n",
    "            # If EOF, this boundary should be at the end of the file\n",
    "            if mini_chunk == b\"\":\n",
    "                chunk_boundaries[bi] = file_size\n",
    "                break\n",
    "\n",
    "            # Find the special token in the mini chunk\n",
    "            found_at = mini_chunk.find(split_special_token)\n",
    "            if found_at != -1:\n",
    "                chunk_boundaries[bi] = initial_position + found_at\n",
    "                break\n",
    "            initial_position += mini_chunk_size\n",
    "\n",
    "    # Make sure all boundaries are unique, but might be fewer than desired_num_chunks\n",
    "    return sorted(set(chunk_boundaries))\n",
    "\n",
    "def pre_tokenize_split(filepath, bound_st, bound_ed, pattern, special_tokens):\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        f.seek(bound_st)\n",
    "        chunk = f.read(bound_ed - bound_st).decode(\"utf-8\", errors=\"ignore\")\n",
    "        special_pat = \"|\".join(map(re.escape, special_tokens))\n",
    "        chunk_set = [s for s in re.split(special_pat, chunk) if s]\n",
    "        corpus_weights = {}\n",
    "        for small_chunk in chunk_set:\n",
    "            splited_text = re.findall(pattern, small_chunk)\n",
    "            for words in splited_text:\n",
    "                data_u8 = words.encode(\"utf-8\")\n",
    "                corpus_weights[data_u8] = corpus_weights.get(data_u8, 0) + 1\n",
    "    return corpus_weights\n",
    "\n",
    "def build_seq_weights(filepath, num_process, special_tokens, PAT):\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        boundaries = find_chunk_boundaries(f, num_processes, b\"<|endoftext|>\")\n",
    "        \n",
    "    parellel_params = [(filepath, start, end, PAT, special_tokens) for start, end in zip(boundaries[:-1], boundaries[1:])]\n",
    "    \n",
    "    with ProcessPoolExecutor(max_workers=num_process) as ex:\n",
    "        results = list(ex.map(pre_tokenize_split, *zip(*parellel_params)))\n",
    "        \n",
    "    seq_weights = {} #{tuple: int(freq)}\n",
    "\n",
    "    for dic in results:\n",
    "        for k,v in dic.items():\n",
    "            tuple_k = tuple(k)\n",
    "            seq_weights[tuple_k] = seq_weights.get(tuple_k,0) + v\n",
    "    \n",
    "    return seq_weights\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4054022a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_pair_cnt(_seq_weights):\n",
    "    _pair_cnt = {}\n",
    "    for k,v in _seq_weights.items():\n",
    "        for ch1,ch2 in zip(k[:-1],k[1:]):\n",
    "            pair = (ch1,ch2)\n",
    "            _pair_cnt[pair] = _pair_cnt.get(pair,0) + v\n",
    "    return _pair_cnt\n",
    "\n",
    "def find_max(_pair_cnt,_token_dict):\n",
    "    maxcnt = -1\n",
    "    maxpair = None\n",
    "    bytes_pair = None\n",
    "    for p,v in _pair_cnt.items():\n",
    "        if v == maxcnt:\n",
    "            bytes_pair_new = _token_dict[p[0]],_token_dict[p[1]]\n",
    "            maxpair = p if bytes_pair_new>bytes_pair else maxpair\n",
    "        if v > maxcnt:\n",
    "            maxcnt = v\n",
    "            maxpair = p\n",
    "            bytes_pair = _token_dict[p[0]],_token_dict[p[1]]\n",
    "    return maxpair,maxcnt\n",
    "\n",
    "def merge_operation(_seq_weights, merge_pair, merge_id):\n",
    "    _seq_weights_copy = {}\n",
    "    for k, v in _seq_weights.items():\n",
    "        _ = 0\n",
    "        new_k = []\n",
    "        while _ < len(k):\n",
    "            if _ + 1 < len(k) and (k[_],k[_+1])==merge_pair:\n",
    "                new_k.append(merge_id)\n",
    "                _ += 2\n",
    "            else:\n",
    "                new_k.append(k[_])\n",
    "                _ += 1\n",
    "        new_k = tuple(new_k)\n",
    "        _seq_weights_copy[new_k] = _seq_weights_copy.get(new_k,0)+v\n",
    "    return _seq_weights_copy\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43746927",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_train_bpe(filepath,vocab_size,special_tokens,PAT,num_processes=4):\n",
    "    token_dict = {i:bytes([i]) for i in range(256)}\n",
    "    merge_list = []\n",
    "    seq_now = build_seq_weights(filepath,num_processes,special_tokens,PAT)\n",
    "    merge_num = vocab_size - len(special_tokens) - 256\n",
    "    token_id = 256\n",
    "    for i in range(merge_num):\n",
    "        pairnow,freq = find_max(build_pair_cnt(seq_now),token_dict)\n",
    "        seq_now = merge_operation(seq_now,pairnow,token_id)\n",
    "        token_dict[token_id] = token_dict[pairnow[0]]+token_dict[pairnow[1]] \n",
    "        merge_list.append((token_dict[pairnow[0]],token_dict[pairnow[1]]))\n",
    "        token_id += 1\n",
    "    for s in special_tokens:\n",
    "        token_dict[token_id] = bytes(s)\n",
    "        token_id += 1\n",
    "    return token_dict, merge_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc33b121",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_dict,merge_list = my_train_bpe(train_text_path,vocab_size,special_tokens,PAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daf0cea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(b' ', b't'),\n",
       " (b' ', b'a'),\n",
       " (b'h', b'e'),\n",
       " (b'i', b'n'),\n",
       " (b' t', b'he'),\n",
       " (b'r', b'e'),\n",
       " (b' ', b'o'),\n",
       " (b' ', b','),\n",
       " (b'e', b'r'),\n",
       " (b' ', b's'),\n",
       " (b'a', b't'),\n",
       " (b' ', b'.'),\n",
       " (b'n', b'd'),\n",
       " (b'i', b's'),\n",
       " (b'o', b'r'),\n",
       " (b' ', b'w'),\n",
       " (b' ', b'c'),\n",
       " (b'o', b'n'),\n",
       " (b' ', b'b'),\n",
       " (b' ', b'f'),\n",
       " (b'o', b'u'),\n",
       " (b'i', b't'),\n",
       " (b'e', b'n'),\n",
       " (b'e', b's'),\n",
       " (b' o', b'f'),\n",
       " (b' ', b'p'),\n",
       " (b'in', b'g'),\n",
       " (b' ', b'in'),\n",
       " (b'e', b'd'),\n",
       " (b'a', b'l'),\n",
       " (b' ', b'm'),\n",
       " (b' a', b'nd'),\n",
       " (b' ', b'd'),\n",
       " (b'a', b'n'),\n",
       " (b'a', b'r'),\n",
       " (b' t', b'o'),\n",
       " (b'o', b'm'),\n",
       " (b' t', b'h'),\n",
       " (b'i', b'c'),\n",
       " (b'i', b'on'),\n",
       " (b' ', b'h'),\n",
       " (b' ', b'l'),\n",
       " (b' ', b'y'),\n",
       " (b' ', b'e'),\n",
       " (b'a', b's'),\n",
       " (b'o', b't'),\n",
       " (b'i', b'l'),\n",
       " (b' ', b'n'),\n",
       " (b' ', b'u'),\n",
       " (b'en', b't'),\n",
       " (b' b', b'e'),\n",
       " (b' ', b'&'),\n",
       " (b' ', b'is'),\n",
       " (b' y', b'ou'),\n",
       " (b'o', b's'),\n",
       " (b' ', b're'),\n",
       " (b'e', b't'),\n",
       " (b' f', b'or'),\n",
       " (b'u', b't'),\n",
       " (b'e', b'l'),\n",
       " (b' ', b'g'),\n",
       " (b'a', b'y'),\n",
       " (b's', b't'),\n",
       " (b'o', b'w'),\n",
       " (b'l', b'e'),\n",
       " (b'c', b'e'),\n",
       " (b'a', b'd'),\n",
       " (b' o', b'n'),\n",
       " (b' ', b'I'),\n",
       " (b'v', b'er'),\n",
       " (b'v', b'e'),\n",
       " (b' ', b'A'),\n",
       " (b'u', b'r'),\n",
       " (b'o', b'l'),\n",
       " (b'c', b't'),\n",
       " (b'q', b'u'),\n",
       " (b' th', b'at'),\n",
       " (b'i', b'm'),\n",
       " (b'al', b'l'),\n",
       " (b'a', b'm'),\n",
       " (b'i', b'g'),\n",
       " (b'c', b'h'),\n",
       " (b'at', b'ion'),\n",
       " (b' ', b'P'),\n",
       " (b'it', b'h'),\n",
       " (b'i', b'r'),\n",
       " (b' ', b'S'),\n",
       " (b' ', b'it'),\n",
       " (b' p', b'r'),\n",
       " (b'a', b'p'),\n",
       " (b' s', b'h'),\n",
       " (b' ', b'C'),\n",
       " (b't', b'h'),\n",
       " (b' c', b'om'),\n",
       " (b' ', b'@'),\n",
       " (b' w', b'h'),\n",
       " (b'-', b'@'),\n",
       " (b' a', b're'),\n",
       " (b' @', b'-@'),\n",
       " (b'n', b't'),\n",
       " (b'i', b'd'),\n",
       " (b' w', b'ith'),\n",
       " (b' a', b'l'),\n",
       " (b'o', b'p'),\n",
       " (b' u', b's'),\n",
       " (b'er', b's'),\n",
       " (b' a', b's'),\n",
       " (b't', b'he'),\n",
       " (b'a', b'nd'),\n",
       " (b'i', b'f'),\n",
       " (b'or', b'd'),\n",
       " (b'o', b'd'),\n",
       " (b' ', b'he'),\n",
       " (b'is', b't'),\n",
       " (b'qu', b'ot'),\n",
       " (b'm', b'ent'),\n",
       " (b' ', b'M'),\n",
       " (b' o', b'r'),\n",
       " (b'o', b're'),\n",
       " (b' ', b'G'),\n",
       " (b' f', b'r'),\n",
       " (b'il', b'l'),\n",
       " (b're', b's'),\n",
       " (b' s', b't'),\n",
       " (b'es', b's'),\n",
       " (b'l', b'd'),\n",
       " (b' th', b'is'),\n",
       " (b' ', b'2'),\n",
       " (b'ar', b't'),\n",
       " (b' ', b';'),\n",
       " (b' ', b'L'),\n",
       " (b'l', b'y'),\n",
       " (b'a', b'in'),\n",
       " (b'u', b'l'),\n",
       " (b' d', b'e'),\n",
       " (b' c', b'on'),\n",
       " (b'es', b't'),\n",
       " (b's', b'e'),\n",
       " (b'ap', b'os'),\n",
       " (b'a', b'g'),\n",
       " (b' fr', b'om'),\n",
       " (b' a', b'n'),\n",
       " (b' w', b'e'),\n",
       " (b' ', b'('),\n",
       " (b'0', b'0'),\n",
       " (b't', b'er'),\n",
       " (b' ', b'E'),\n",
       " (b'e', b'm'),\n",
       " (b'a', b've'),\n",
       " (b' n', b'ot'),\n",
       " (b' ', b')'),\n",
       " (b' ', b'1'),\n",
       " (b' you', b'r'),\n",
       " (b' c', b'an'),\n",
       " (b'o', b'c'),\n",
       " (b' b', b'y'),\n",
       " (b' ', b'D'),\n",
       " (b' n', b'e'),\n",
       " (b' ', b'v'),\n",
       " (b'ig', b'h'),\n",
       " (b'ic', b'h'),\n",
       " (b' al', b'l'),\n",
       " (b'r', b'i'),\n",
       " (b' u', b'p'),\n",
       " (b' ', b'r'),\n",
       " (b' ', b'W'),\n",
       " (b'b', b'le'),\n",
       " (b' the', b'y'),\n",
       " (b' ', b'B'),\n",
       " (b' y', b'e'),\n",
       " (b'u', b'n'),\n",
       " (b' wh', b'ich'),\n",
       " (b' ', b'O'),\n",
       " (b'k', b'e'),\n",
       " (b' w', b'or'),\n",
       " (b' s', b'u'),\n",
       " (b' ', b'F'),\n",
       " (b' ', b'H'),\n",
       " (b' h', b'ave'),\n",
       " (b' sh', b'all'),\n",
       " (b'at', b'e'),\n",
       " (b' c', b'h'),\n",
       " (b'e', b'ct'),\n",
       " (b'it', b'y'),\n",
       " (b' s', b'p'),\n",
       " (b'igh', b't'),\n",
       " (b'res', b's'),\n",
       " (b' w', b'ill'),\n",
       " (b' com', b'p'),\n",
       " (b'or', b't'),\n",
       " (b'an', b't'),\n",
       " (b' &', b'#'),\n",
       " (b'i', b've'),\n",
       " (b'a', b're'),\n",
       " (b'.', b'.'),\n",
       " (b' A', b'nd'),\n",
       " (b' e', b'x'),\n",
       " (b' .', b'..'),\n",
       " (b'as', b't'),\n",
       " (b'1', b'2'),\n",
       " (b' ', b'T'),\n",
       " (b'ou', b'ld'),\n",
       " (b'v', b'en'),\n",
       " (b' t', b'r'),\n",
       " (b'u', b'st'),\n",
       " (b'u', b'm'),\n",
       " (b'ou', b't'),\n",
       " (b' u', b'nt'),\n",
       " (b'c', b'om'),\n",
       " (b' s', b'e'),\n",
       " (b'f', b't'),\n",
       " (b'l', b'o'),\n",
       " (b're', b'e'),\n",
       " (b'os', b't'),\n",
       " (b'is', b'h'),\n",
       " (b'ion', b's'),\n",
       " (b' unt', b'o'),\n",
       " (b'12', b'4'),\n",
       " (b'i', b'z'),\n",
       " (b' pr', b'o'),\n",
       " (b'm', b'er'),\n",
       " (b'ing', b's'),\n",
       " (b' a', b'c'),\n",
       " (b'at', b'ed'),\n",
       " (b'th', b'is'),\n",
       " (b'a', b'c'),\n",
       " (b'l', b'u'),\n",
       " (b'e', b're'),\n",
       " (b' m', b'an'),\n",
       " (b'f', b'or'),\n",
       " (b' m', b'y'),\n",
       " (b' a', b't'),\n",
       " (b'i', b'es'),\n",
       " (b'ag', b'e'),\n",
       " (b'r', b'ou'),\n",
       " (b'an', b's'),\n",
       " (b'in', b'd'),\n",
       " (b'p', b'p'),\n",
       " (b' wor', b'k'),\n",
       " (b'he', b're'),\n",
       " (b'f', b'ore'),\n",
       " (b' s', b'it'),\n",
       " (b' ', b'ver')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336-basics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
